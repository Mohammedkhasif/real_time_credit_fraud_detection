{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UhedaGkudAOo",
        "outputId": "77608f89-945c-4870-cefc-9870ab1221a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. LOADING AND EXPLORING THE DATASET\n",
            "--------------------------------------------------\n",
            "Loading dataset...\n",
            "Dataset loaded successfully.\n",
            "\n",
            "Dataset Information:\n",
            "Shape: (107046, 31)\n",
            "Features: ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount', 'Class']\n",
            "\n",
            "Sample data:\n",
            "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
            "0     0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
            "1     0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
            "2     1 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
            "3     1 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
            "4     2 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
            "\n",
            "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
            "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
            "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
            "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
            "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
            "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
            "\n",
            "        V26       V27       V28  Amount  Class  \n",
            "0 -0.189115  0.133558 -0.021053  149.62    0.0  \n",
            "1  0.125895 -0.008983  0.014724    2.69    0.0  \n",
            "2 -0.139097 -0.055353 -0.059752  378.66    0.0  \n",
            "3 -0.221929  0.062723  0.061458  123.50    0.0  \n",
            "4  0.502292  0.219422  0.215153   69.99    0.0  \n",
            "\n",
            "[5 rows x 31 columns]\n",
            "\n",
            "Missing values:\n",
            "3\n",
            "\n",
            "Class distribution:\n",
            "Class\n",
            "0.0    106810\n",
            "1.0       235\n",
            "Name: count, dtype: int64\n",
            "Fraud percentage: 0.2195%\n",
            "\n",
            "2. EXPLORATORY DATA ANALYSIS\n",
            "--------------------------------------------------\n",
            "Class distribution plot saved.\n",
            "Transaction amount distribution plot saved.\n",
            "Correlation matrix plot saved.\n",
            "\n",
            "3. DATA PREPROCESSING\n",
            "--------------------------------------------------\n",
            "Training set: 85636 samples\n",
            "Testing set: 21409 samples\n",
            "Features scaled successfully.\n",
            "\n",
            "Handling class imbalance with SMOTE...\n",
            "Original training set shape: (85636, 30)\n",
            "Resampled training set shape: (170896, 30)\n",
            "Class distribution after SMOTE: {0.0: 85448, 1.0: 85448}\n",
            "\n",
            "4. MODEL BUILDING AND EVALUATION\n",
            "--------------------------------------------------\n",
            "\n",
            "4.1 LOGISTIC REGRESSION\n",
            "--------------------------------------------------\n",
            "\n",
            "Evaluating Logistic Regression...\n",
            "Training time: 40.17 seconds\n",
            "Accuracy: 0.9902\n",
            "Precision: 0.1734\n",
            "Recall: 0.9149\n",
            "F1 Score: 0.2915\n",
            "AUC-ROC: 0.9824\n",
            "PR AUC: 0.7412\n",
            "Confusion Matrix:\n",
            "[[21157   205]\n",
            " [    4    43]]\n",
            "Logistic Regression evaluation plots saved.\n",
            "\n",
            "4.2 RANDOM FOREST\n",
            "--------------------------------------------------\n",
            "\n",
            "Evaluating Random Forest...\n",
            "Training time: 67.86 seconds\n",
            "Accuracy: 0.9990\n",
            "Precision: 0.7167\n",
            "Recall: 0.9149\n",
            "F1 Score: 0.8037\n",
            "AUC-ROC: 0.9873\n",
            "PR AUC: 0.9030\n",
            "Confusion Matrix:\n",
            "[[21345    17]\n",
            " [    4    43]]\n",
            "Random Forest evaluation plots saved.\n",
            "\n",
            "4.3 XGBOOST\n",
            "--------------------------------------------------\n",
            "\n",
            "Evaluating XGBoost...\n",
            "Training time: 2.97 seconds\n",
            "Accuracy: 0.9961\n",
            "Precision: 0.3520\n",
            "Recall: 0.9362\n",
            "F1 Score: 0.5116\n",
            "AUC-ROC: 0.9793\n",
            "PR AUC: 0.9129\n",
            "Confusion Matrix:\n",
            "[[21281    81]\n",
            " [    3    44]]\n",
            "XGBoost evaluation plots saved.\n",
            "\n",
            "5. MODEL COMPARISON\n",
            "--------------------------------------------------\n",
            "\n",
            "Model Comparison:\n",
            "                 Model  Accuracy  Precision    Recall  F1 Score   AUC-ROC  \\\n",
            "0  Logistic Regression  0.990238   0.173387  0.914894  0.291525  0.982405   \n",
            "1        Random Forest  0.999019   0.716667  0.914894  0.803738  0.987260   \n",
            "2              XGBoost  0.996076   0.352000  0.936170  0.511628  0.979326   \n",
            "\n",
            "     PR AUC  Training Time  \n",
            "0  0.741222      40.169314  \n",
            "1  0.902996      67.859777  \n",
            "2  0.912886       2.971851  \n",
            "Model comparison plot saved.\n",
            "\n",
            "6. FEATURE IMPORTANCE ANALYSIS\n",
            "--------------------------------------------------\n",
            "Top 10 Important Features (Random Forest):\n",
            "   Feature  Importance\n",
            "14     V14    0.227181\n",
            "10     V10    0.125212\n",
            "12     V12    0.108741\n",
            "3       V3    0.101410\n",
            "17     V17    0.099847\n",
            "4       V4    0.082487\n",
            "16     V16    0.046184\n",
            "2       V2    0.041238\n",
            "11     V11    0.036929\n",
            "27     V27    0.019210\n",
            "Random Forest feature importance plot saved.\n",
            "\n",
            "Top 10 Important Features (XGBoost):\n",
            "   Feature  Importance\n",
            "14     V14    0.341871\n",
            "4       V4    0.120749\n",
            "3       V3    0.079971\n",
            "10     V10    0.056890\n",
            "1       V1    0.036773\n",
            "29  Amount    0.031384\n",
            "13     V13    0.023278\n",
            "7       V7    0.022046\n",
            "11     V11    0.022006\n",
            "26     V26    0.021793\n",
            "XGBoost feature importance plot saved.\n",
            "\n",
            "7. THRESHOLD OPTIMIZATION\n",
            "--------------------------------------------------\n",
            "Threshold optimization plot saved.\n",
            "Optimal threshold for F1 score: 0.85\n",
            "Optimal F1 score: 0.6992\n",
            "Corresponding Precision: 0.5658\n",
            "Corresponding Recall: 0.9149\n",
            "\n",
            "Metrics with optimal threshold:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00     21362\n",
            "         1.0       0.57      0.91      0.70        47\n",
            "\n",
            "    accuracy                           1.00     21409\n",
            "   macro avg       0.78      0.96      0.85     21409\n",
            "weighted avg       1.00      1.00      1.00     21409\n",
            "\n",
            "\n",
            "8. COST-SENSITIVE EVALUATION\n",
            "--------------------------------------------------\n",
            "Cost threshold plot saved.\n",
            "Cost-optimal threshold: 0.80\n",
            "Minimum cost: 690.00\n",
            "\n",
            "Metrics with cost-optimal threshold:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00     21362\n",
            "         1.0       0.53      0.94      0.68        47\n",
            "\n",
            "    accuracy                           1.00     21409\n",
            "   macro avg       0.76      0.97      0.84     21409\n",
            "weighted avg       1.00      1.00      1.00     21409\n",
            "\n",
            "Confusion Matrix with cost-optimal threshold:\n",
            "[[21323    39]\n",
            " [    3    44]]\n",
            "\n",
            "9. FINAL MODEL SELECTION\n",
            "--------------------------------------------------\n",
            "Based on various metrics and considerations, we recommend:\n",
            "Best model by F1 Score: Random Forest with F1 = 0.8037\n",
            "Best model by PR AUC: XGBoost with PR AUC = 0.9129\n",
            "Best overall balanced model: Random Forest with Balanced Score = 0.8237\n",
            "\n",
            "FINAL RECOMMENDATION:\n",
            "We recommend using Random Forest with a threshold of 0.80 optimized for minimizing business costs.\n",
            "This model provides the best balance of precision and recall while minimizing the financial impact of fraud.\n",
            "\n",
            "10. DEPLOYMENT CONSIDERATIONS\n",
            "--------------------------------------------------\n",
            "\n",
            "Deployment Recommendations:\n",
            "1. Model Serialization: Save the final model using joblib or pickle for deployment.\n",
            "2. Real-time Inferencing: Consider implementing an API for real-time fraud detection.\n",
            "3. Monitoring: Establish metrics to monitor model performance in production.\n",
            "4. Retraining Strategy: Set up periodic retraining to adapt to new fraud patterns.\n",
            "5. Explainability: Implement SHAP or LIME for transaction-level explanations.\n",
            "6. Alerting System: Set up an alerting system for high-confidence fraud predictions.\n",
            "7. Fallback Mechanisms: Implement fallback rules for when the model is uncertain.\n",
            "\n",
            "Final model saved as 'xgb_fraud_detection_model.pkl'\n",
            "\n",
            "CREDIT CARD FRAUD DETECTION PROJECT COMPLETED!\n",
            "--------------------------------------------------\n",
            "\n",
            "# Sample code for model deployment (REST API using Flask)\n",
            "'''\n",
            "from flask import Flask, request, jsonify\n",
            "import joblib\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "\n",
            "app = Flask(__name__)\n",
            "\n",
            "# Load the model\n",
            "model = joblib.load('xgb_fraud_detection_model.pkl')\n",
            "scaler = joblib.load('scaler.pkl')  # You would need to save this separately\n",
            "\n",
            "@app.route('/predict', methods=['POST'])\n",
            "def predict():\n",
            "    # Get data from POST request\n",
            "    data = request.json\n",
            "    \n",
            "    # Preprocess the data (same as in training)\n",
            "    df = pd.DataFrame([data])\n",
            "    \n",
            "    # Scale the features\n",
            "    features_to_scale = [col for col in df.columns if col != 'Time']\n",
            "    df[features_to_scale] = scaler.transform(df[features_to_scale])\n",
            "    \n",
            "    # Make prediction\n",
            "    probability = model.predict_proba(df)[0][1]\n",
            "    is_fraud = probability >= 0.3  # Use the cost-optimal threshold\n",
            "    \n",
            "    # Return the result\n",
            "    return jsonify({\n",
            "        'probability': float(probability),\n",
            "        'is_fraud': bool(is_fraud),\n",
            "        'threshold': 0.3\n",
            "    })\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    app.run(debug=True)\n",
            "'''\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x1000 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_curve, auc, roc_curve, roc_auc_score, f1_score, recall_score, precision_score\n",
        "from sklearn.utils import resample\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import xgboost as xgb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import time\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# 1. DATA LOADING AND EXPLORATION\n",
        "print(\"1. LOADING AND EXPLORING THE DATASET\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Load the dataset (assuming you have a CSV file)\n",
        "# For this example, we'll use a placeholder for loading data\n",
        "# In a real project, replace this with your actual dataset\n",
        "# Example: df = pd.read_csv('creditcard.csv')\n",
        "\n",
        "# Simulating data loading (in a real project, you would load actual data)\n",
        "print(\"Loading dataset...\")\n",
        "try:\n",
        "    # Try to load the dataset if available\n",
        "    df = pd.read_csv('/content/creditcard.csv')\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "except:\n",
        "    print(\"Dataset not found. Creating a synthetic dataset for demonstration.\")\n",
        "    # Create a synthetic dataset for demonstration\n",
        "    n_samples = 10000\n",
        "    n_features = 30\n",
        "\n",
        "    # Generate synthetic features\n",
        "    X = np.random.randn(n_samples, n_features)\n",
        "\n",
        "    # Create imbalanced classes (0.5% fraud)\n",
        "    n_fraud = int(0.005 * n_samples)\n",
        "    y = np.zeros(n_samples)\n",
        "    fraud_indices = np.random.choice(range(n_samples), size=n_fraud, replace=False)\n",
        "    y[fraud_indices] = 1\n",
        "\n",
        "    # Create dataframe\n",
        "    feature_names = [f'V{i}' for i in range(1, n_features+1)]\n",
        "    df = pd.DataFrame(X, columns=feature_names)\n",
        "    df['Amount'] = np.abs(np.random.randn(n_samples) * 500 + 100)  # Transaction amount\n",
        "    df['Time'] = np.random.randint(0, 172800, size=n_samples)  # Transaction time in seconds\n",
        "    df['Class'] = y  # Target variable\n",
        "\n",
        "    print(f\"Created synthetic dataset with {n_samples} samples and {n_fraud} fraudulent transactions.\")\n",
        "\n",
        "# Display basic information about the dataset\n",
        "print(\"\\nDataset Information:\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"Features: {df.columns.tolist()}\")\n",
        "print(\"\\nSample data:\")\n",
        "print(df.head())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values:\")\n",
        "print(df.isnull().sum().sum())\n",
        "\n",
        "# Data distribution\n",
        "print(\"\\nClass distribution:\")\n",
        "print(df['Class'].value_counts())\n",
        "print(f\"Fraud percentage: {df['Class'].mean() * 100:.4f}%\")\n",
        "\n",
        "# 2. EXPLORATORY DATA ANALYSIS\n",
        "print(\"\\n2. EXPLORATORY DATA ANALYSIS\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Visualize class distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x='Class', data=df)\n",
        "plt.title('Class Distribution (0: Normal, 1: Fraud)')\n",
        "plt.savefig('class_distribution.png')\n",
        "plt.close()\n",
        "print(\"Class distribution plot saved.\")\n",
        "\n",
        "# Analyze transaction amount by class\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df[df['Class'] == 0]['Amount'], bins=50, kde=True)\n",
        "plt.title('Transaction Amount - Normal')\n",
        "plt.xlim([0, 500])\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(df[df['Class'] == 1]['Amount'], bins=50, kde=True, color='red')\n",
        "plt.title('Transaction Amount - Fraud')\n",
        "plt.xlim([0, 500])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('amount_distribution.png')\n",
        "plt.close()\n",
        "print(\"Transaction amount distribution plot saved.\")\n",
        "\n",
        "# Correlation matrix of features\n",
        "plt.figure(figsize=(15, 12))\n",
        "corr_matrix = df.corr()\n",
        "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "sns.heatmap(corr_matrix, mask=mask, annot=False, cmap='coolwarm', linewidths=0.5)\n",
        "plt.title('Correlation Matrix')\n",
        "plt.tight_layout()\n",
        "plt.savefig('correlation_matrix.png')\n",
        "plt.close()\n",
        "print(\"Correlation matrix plot saved.\")\n",
        "\n",
        "\n",
        "# 3. DATA PREPROCESSING\n",
        "print(\"\\n3. DATA PREPROCESSING\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "# Remove rows with NaN values in the target variable 'Class'\n",
        "df = df.dropna(subset=['Class'])\n",
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
        "\n",
        "# ... (rest of your code remains the same)\n",
        "# Scale the features (except Time if it exists)\n",
        "scaler = StandardScaler()\n",
        "if 'Time' in X_train.columns:\n",
        "    features_to_scale = X_train.drop('Time', axis=1).columns\n",
        "    X_train[features_to_scale] = scaler.fit_transform(X_train[features_to_scale])\n",
        "    X_test[features_to_scale] = scaler.transform(X_test[features_to_scale])\n",
        "else:\n",
        "    X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
        "    X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
        "\n",
        "print(\"Features scaled successfully.\")\n",
        "\n",
        "# Handle imbalanced data using SMOTE\n",
        "print(\"\\nHandling class imbalance with SMOTE...\")\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print(f\"Original training set shape: {X_train.shape}\")\n",
        "print(f\"Resampled training set shape: {X_train_resampled.shape}\")\n",
        "print(f\"Class distribution after SMOTE: {pd.Series(y_train_resampled).value_counts().to_dict()}\")\n",
        "\n",
        "# 4. MODEL BUILDING AND EVALUATION\n",
        "print(\"\\n4. MODEL BUILDING AND EVALUATION\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Function to evaluate models\n",
        "def evaluate_model(model, X_train, y_train, X_test, y_test, model_name):\n",
        "    print(f\"\\nEvaluating {model_name}...\")\n",
        "\n",
        "    # Train the model and measure training time\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    # AUC-ROC\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Precision-Recall AUC\n",
        "    precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred_proba)\n",
        "    pr_auc = auc(recall_curve, precision_curve)\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"Training time: {training_time:.2f} seconds\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"AUC-ROC: {roc_auc:.4f}\")\n",
        "    print(f\"PR AUC: {pr_auc:.4f}\")\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "    # Plot ROC curve\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # ROC Curve\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.4f}')\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'ROC Curve - {model_name}')\n",
        "    plt.legend(loc='lower right')\n",
        "\n",
        "    # PR Curve\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(recall_curve, precision_curve, label=f'PR AUC = {pr_auc:.4f}')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title(f'Precision-Recall Curve - {model_name}')\n",
        "    plt.legend(loc='lower left')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{model_name.lower().replace(\" \", \"_\")}_curves.png')\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"{model_name} evaluation plots saved.\")\n",
        "\n",
        "    # Return metrics for comparison\n",
        "    return {\n",
        "        'Model': model_name,\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1 Score': f1,\n",
        "        'AUC-ROC': roc_auc,\n",
        "        'PR AUC': pr_auc,\n",
        "        'Training Time': training_time\n",
        "    }\n",
        "\n",
        "# 4.1 Logistic Regression\n",
        "print(\"\\n4.1 LOGISTIC REGRESSION\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Initialize and evaluate Logistic Regression model\n",
        "log_reg = LogisticRegression(C=1.0, class_weight='balanced', max_iter=1000, random_state=42)\n",
        "log_reg_metrics = evaluate_model(log_reg, X_train_resampled, y_train_resampled, X_test, y_test, \"Logistic Regression\")\n",
        "\n",
        "# 4.2 Random Forest\n",
        "print(\"\\n4.2 RANDOM FOREST\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Initialize and evaluate Random Forest model\n",
        "rf = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=10,\n",
        "                           class_weight='balanced', random_state=42, n_jobs=-1)\n",
        "rf_metrics = evaluate_model(rf, X_train_resampled, y_train_resampled, X_test, y_test, \"Random Forest\")\n",
        "\n",
        "# 4.3 XGBoost\n",
        "print(\"\\n4.3 XGBOOST\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Initialize and evaluate XGBoost model\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=5,\n",
        "                            scale_pos_weight=99, # Adjust this based on class imbalance ratio\n",
        "                            objective='binary:logistic', random_state=42, n_jobs=-1)\n",
        "xgb_metrics = evaluate_model(xgb_model, X_train_resampled, y_train_resampled, X_test, y_test, \"XGBoost\")\n",
        "\n",
        "# 5. MODEL COMPARISON\n",
        "print(\"\\n5. MODEL COMPARISON\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Collect all metrics\n",
        "models_metrics = [log_reg_metrics, rf_metrics, xgb_metrics]\n",
        "metrics_df = pd.DataFrame(models_metrics)\n",
        "print(\"\\nModel Comparison:\")\n",
        "print(metrics_df)\n",
        "\n",
        "# Plot metrics comparison\n",
        "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC-ROC', 'PR AUC']\n",
        "metrics_df_plot = metrics_df.set_index('Model')[metrics_to_plot]\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "metrics_df_plot.plot(kind='bar', rot=0, figsize=(15, 8))\n",
        "plt.title('Model Performance Comparison')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim([0, 1])\n",
        "plt.legend(loc='lower right')\n",
        "plt.tight_layout()\n",
        "plt.savefig('model_comparison.png')\n",
        "plt.close()\n",
        "print(\"Model comparison plot saved.\")\n",
        "\n",
        "# Feature Importance Analysis\n",
        "print(\"\\n6. FEATURE IMPORTANCE ANALYSIS\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Random Forest Feature Importance\n",
        "plt.figure(figsize=(12, 8))\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': rf.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"Top 10 Important Features (Random Forest):\")\n",
        "print(feature_importance.head(10))\n",
        "\n",
        "sns.barplot(x='Importance', y='Feature', data=feature_importance.head(15))\n",
        "plt.title('Feature Importance (Random Forest)')\n",
        "plt.tight_layout()\n",
        "plt.savefig('rf_feature_importance.png')\n",
        "plt.close()\n",
        "print(\"Random Forest feature importance plot saved.\")\n",
        "\n",
        "# XGBoost Feature Importance\n",
        "plt.figure(figsize=(12, 8))\n",
        "xgb_importance = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': xgb_model.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 Important Features (XGBoost):\")\n",
        "print(xgb_importance.head(10))\n",
        "\n",
        "sns.barplot(x='Importance', y='Feature', data=xgb_importance.head(15))\n",
        "plt.title('Feature Importance (XGBoost)')\n",
        "plt.tight_layout()\n",
        "plt.savefig('xgb_feature_importance.png')\n",
        "plt.close()\n",
        "print(\"XGBoost feature importance plot saved.\")\n",
        "\n",
        "# 7. THRESHOLD OPTIMIZATION\n",
        "print(\"\\n7. THRESHOLD OPTIMIZATION\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Finding the optimal threshold for the best model (using XGBoost as an example)\n",
        "y_pred_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "thresholds = np.arange(0.1, 0.9, 0.05)\n",
        "f1_scores = []\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "\n",
        "for threshold in thresholds:\n",
        "    y_pred_threshold = (y_pred_proba_xgb >= threshold).astype(int)\n",
        "    f1_scores.append(f1_score(y_test, y_pred_threshold))\n",
        "    precision_scores.append(precision_score(y_test, y_pred_threshold))\n",
        "    recall_scores.append(recall_score(y_test, y_pred_threshold))\n",
        "\n",
        "# Plot threshold vs metrics\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(thresholds, f1_scores, label='F1 Score')\n",
        "plt.plot(thresholds, precision_scores, label='Precision')\n",
        "plt.plot(thresholds, recall_scores, label='Recall')\n",
        "plt.xlabel('Threshold')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Metrics vs. Classification Threshold')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('threshold_optimization.png')\n",
        "plt.close()\n",
        "print(\"Threshold optimization plot saved.\")\n",
        "\n",
        "# Find optimal threshold for F1 score\n",
        "optimal_idx = np.argmax(f1_scores)\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "print(f\"Optimal threshold for F1 score: {optimal_threshold:.2f}\")\n",
        "print(f\"Optimal F1 score: {f1_scores[optimal_idx]:.4f}\")\n",
        "print(f\"Corresponding Precision: {precision_scores[optimal_idx]:.4f}\")\n",
        "print(f\"Corresponding Recall: {recall_scores[optimal_idx]:.4f}\")\n",
        "\n",
        "# Apply optimal threshold\n",
        "y_pred_optimal = (y_pred_proba_xgb >= optimal_threshold).astype(int)\n",
        "print(\"\\nMetrics with optimal threshold:\")\n",
        "print(classification_report(y_test, y_pred_optimal))\n",
        "\n",
        "# 8. COST-SENSITIVE EVALUATION\n",
        "print(\"\\n8. COST-SENSITIVE EVALUATION\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Define cost matrix (example values)\n",
        "# False Negative (missing fraud) is much more costly than False Positive\n",
        "cost_fn = 100  # Cost of missing a fraud\n",
        "cost_fp = 10   # Cost of falsely flagging a normal transaction\n",
        "\n",
        "def calculate_cost(y_true, y_pred, cost_fn=100, cost_fp=10):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    fn = cm[1][0]  # False Negatives\n",
        "    fp = cm[0][1]  # False Positives\n",
        "    total_cost = fn * cost_fn + fp * cost_fp\n",
        "    return total_cost\n",
        "\n",
        "# Calculate costs for different thresholds\n",
        "cost_per_threshold = []\n",
        "for threshold in thresholds:\n",
        "    y_pred_threshold = (y_pred_proba_xgb >= threshold).astype(int)\n",
        "    cost = calculate_cost(y_test, y_pred_threshold, cost_fn, cost_fp)\n",
        "    cost_per_threshold.append(cost)\n",
        "\n",
        "# Plot cost vs threshold\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(thresholds, cost_per_threshold)\n",
        "plt.xlabel('Threshold')\n",
        "plt.ylabel('Total Cost')\n",
        "plt.title('Cost vs. Classification Threshold')\n",
        "plt.grid(True)\n",
        "plt.savefig('cost_threshold.png')\n",
        "plt.close()\n",
        "print(\"Cost threshold plot saved.\")\n",
        "\n",
        "# Find optimal threshold for cost\n",
        "min_cost_idx = np.argmin(cost_per_threshold)\n",
        "cost_optimal_threshold = thresholds[min_cost_idx]\n",
        "print(f\"Cost-optimal threshold: {cost_optimal_threshold:.2f}\")\n",
        "print(f\"Minimum cost: {cost_per_threshold[min_cost_idx]:.2f}\")\n",
        "\n",
        "# Apply cost-optimal threshold\n",
        "y_pred_cost_optimal = (y_pred_proba_xgb >= cost_optimal_threshold).astype(int)\n",
        "print(\"\\nMetrics with cost-optimal threshold:\")\n",
        "print(classification_report(y_test, y_pred_cost_optimal))\n",
        "print(f\"Confusion Matrix with cost-optimal threshold:\")\n",
        "print(confusion_matrix(y_test, y_pred_cost_optimal))\n",
        "\n",
        "# 9. FINAL MODEL SELECTION\n",
        "print(\"\\n9. FINAL MODEL SELECTION\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Choose the best model based on comprehensive evaluation\n",
        "print(\"Based on various metrics and considerations, we recommend:\")\n",
        "\n",
        "# Find the model with the highest F1 score\n",
        "best_model_f1 = metrics_df.loc[metrics_df['F1 Score'].idxmax()]\n",
        "print(f\"Best model by F1 Score: {best_model_f1['Model']} with F1 = {best_model_f1['F1 Score']:.4f}\")\n",
        "\n",
        "# Find the model with the highest PR AUC\n",
        "best_model_pr_auc = metrics_df.loc[metrics_df['PR AUC'].idxmax()]\n",
        "print(f\"Best model by PR AUC: {best_model_pr_auc['Model']} with PR AUC = {best_model_pr_auc['PR AUC']:.4f}\")\n",
        "\n",
        "# Find the model with the best balance of metrics (using mean of normalized metrics)\n",
        "# Normalize all performance metrics\n",
        "metrics_to_normalize = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC-ROC', 'PR AUC']\n",
        "normalized_df = metrics_df.copy()\n",
        "for metric in metrics_to_normalize:\n",
        "    max_val = normalized_df[metric].max()\n",
        "    min_val = normalized_df[metric].min()\n",
        "    if max_val != min_val:\n",
        "        normalized_df[metric] = (normalized_df[metric] - min_val) / (max_val - min_val)\n",
        "    else:\n",
        "        normalized_df[metric] = 1.0\n",
        "\n",
        "# Add a balanced score (mean of normalized metrics)\n",
        "normalized_df['Balanced Score'] = normalized_df[metrics_to_normalize].mean(axis=1)\n",
        "best_balanced_model = normalized_df.loc[normalized_df['Balanced Score'].idxmax()]\n",
        "print(f\"Best overall balanced model: {best_balanced_model['Model']} with Balanced Score = {best_balanced_model['Balanced Score']:.4f}\")\n",
        "\n",
        "# Final recommendation\n",
        "print(\"\\nFINAL RECOMMENDATION:\")\n",
        "print(f\"We recommend using {best_balanced_model['Model']} with a threshold of {cost_optimal_threshold:.2f} optimized for minimizing business costs.\")\n",
        "print(\"This model provides the best balance of precision and recall while minimizing the financial impact of fraud.\")\n",
        "\n",
        "# 10. DEPLOYMENT CONSIDERATIONS\n",
        "print(\"\\n10. DEPLOYMENT CONSIDERATIONS\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"\"\"\n",
        "Deployment Recommendations:\n",
        "1. Model Serialization: Save the final model using joblib or pickle for deployment.\n",
        "2. Real-time Inferencing: Consider implementing an API for real-time fraud detection.\n",
        "3. Monitoring: Establish metrics to monitor model performance in production.\n",
        "4. Retraining Strategy: Set up periodic retraining to adapt to new fraud patterns.\n",
        "5. Explainability: Implement SHAP or LIME for transaction-level explanations.\n",
        "6. Alerting System: Set up an alerting system for high-confidence fraud predictions.\n",
        "7. Fallback Mechanisms: Implement fallback rules for when the model is uncertain.\n",
        "\"\"\")\n",
        "\n",
        "# Save the final model (XGBoost in this example)\n",
        "import joblib\n",
        "joblib.dump(xgb_model, 'xgb_fraud_detection_model.pkl')\n",
        "print(\"Final model saved as 'xgb_fraud_detection_model.pkl'\")\n",
        "\n",
        "print(\"\\nCREDIT CARD FRAUD DETECTION PROJECT COMPLETED!\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Sample code for model deployment (for demonstration)\n",
        "print(\"\"\"\n",
        "# Sample code for model deployment (REST API using Flask)\n",
        "'''\n",
        "from flask import Flask, request, jsonify\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load the model\n",
        "model = joblib.load('xgb_fraud_detection_model.pkl')\n",
        "scaler = joblib.load('scaler.pkl')  # You would need to save this separately\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    # Get data from POST request\n",
        "    data = request.json\n",
        "\n",
        "    # Preprocess the data (same as in training)\n",
        "    df = pd.DataFrame([data])\n",
        "\n",
        "    # Scale the features\n",
        "    features_to_scale = [col for col in df.columns if col != 'Time']\n",
        "    df[features_to_scale] = scaler.transform(df[features_to_scale])\n",
        "\n",
        "    # Make prediction\n",
        "    probability = model.predict_proba(df)[0][1]\n",
        "    is_fraud = probability >= 0.3  # Use the cost-optimal threshold\n",
        "\n",
        "    # Return the result\n",
        "    return jsonify({\n",
        "        'probability': float(probability),\n",
        "        'is_fraud': bool(is_fraud),\n",
        "        'threshold': 0.3\n",
        "    })\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n",
        "'''\n",
        "\"\"\")"
      ]
    }
  ]
}